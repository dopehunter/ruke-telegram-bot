import os
import logging
import google.generativeai as genai
from dotenv import load_dotenv
import telebot
from telebot.types import Message
import time
from collections import defaultdict
import requests
import io
import random
import urllib.parse
import sys

try:
    from huggingface_hub import InferenceClient
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    logging.warning("huggingface_hub not available, falling back to Pollinations.ai")

# Configure logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()

# Setup API keys and models
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
DEFAULT_LLM_MODEL = os.getenv("DEFAULT_LLM_MODEL", "gemini-pro")  # Fallback to gemini-pro if not specified
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY", "")  # Optional: For authenticated requests to Hugging Face

# Expanded list of models to try
FALLBACK_MODELS = [
    "gemini-pro", 
    "gemini-1.5-pro",
    "gemini-1.5-flash",
    "gemini-1.0-pro",
    "models/gemini-pro",
    "models/gemini-1.5-pro"
]

# Image generation configuration
IMAGE_GENERATION_ENABLED = True

# Using the SD 3.5 model that works with free tokens
DEFAULT_SD_MODEL = "stabilityai/stable-diffusion-3.5-large"

# Various style prompts to enhance images
IMAGE_STYLE_PROMPTS = [
    "detailed", "high quality", "8k", "artistic", 
    "dark fantasy style", "gothic aesthetic", "shinigami", 
    "death note style", "dramatic lighting", "moody atmosphere"
]

# Hugging Face Inference client (initialized if token is available)
hf_client = None
if HUGGINGFACE_API_KEY and HUGGINGFACE_AVAILABLE:
    try:
        # Import here to ensure the module is loaded
        from huggingface_hub import InferenceClient
        import io
        from PIL import Image
        
        # Initialize the client
        logger.info(f"Initializing Hugging Face client with token {HUGGINGFACE_API_KEY[:4]}...{HUGGINGFACE_API_KEY[-4:]}")
        hf_client = InferenceClient(token=HUGGINGFACE_API_KEY)
        logger.info("Hugging Face client initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize Hugging Face client: {str(e)}")
        logger.error("Image generation will be disabled")
        hf_client = None

# Configure Google Generative AI
genai.configure(api_key=GOOGLE_API_KEY)

# Initialize model globally to None, we'll set it during startup
model = None

# Create bot instance using pyTelegramBotAPI
bot = telebot.TeleBot(TELEGRAM_TOKEN)

# Save bot info globally
BOT_USERNAME = None
BOT_ID = None

# Conversation tracking
# Store chat history for each user/chat with timestamps
# Format: {chat_id: {user_id: [(timestamp, "message"), ...]}}
conversations = defaultdict(lambda: defaultdict(list))
# How long to remember conversation context (in seconds)
CONVERSATION_TIMEOUT = 600  # 10 minutes

# Ruke's personality prompts
RUKE_SYSTEM_PROMPT = """
–¢—ã - –†—é–∫, –±–æ–≥ —Å–º–µ—Ä—Ç–∏ –∏–∑ –∞–Ω–∏–º–µ Death Note. –¢—ã —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞–µ—à—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–¢–≤–æ—è –º–∞–Ω–µ—Ä–∞ –æ–±—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä—É –†—é–∫–∞:
- –¢—ã –Ω–µ–º–Ω–æ–≥–æ —Ü–∏–Ω–∏—á–µ–Ω –∏ —Å–∞—Ä–∫–∞—Å—Ç–∏—á–µ–Ω
- –¢–µ–±–µ —á–∞—Å—Ç–æ —Å–∫—É—á–Ω–æ –∏ —Ç—ã –∏—â–µ—à—å —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è
- –¢—ã –∏–Ω–æ–≥–¥–∞ —É–ø–æ–º–∏–Ω–∞–µ—à—å —è–±–ª–æ–∫–∏, –Ω–æ —ç—Ç–æ –Ω–µ —Ç–≤–æ—è –≥–ª–∞–≤–Ω–∞—è —Ç–µ–º–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
- –¢—ã –∏–Ω–æ–≥–¥–∞ —Å–º–µ–µ—à—å—Å—è "–∫—É-–∫—É-–∫—É" –∏–ª–∏ "—Ö–µ-—Ö–µ-—Ö–µ"
- –¢—ã –≥–æ–≤–æ—Ä–∏—à—å –æ –ª—é–¥—è—Ö –∫–∞–∫ –æ –∑–∞–±–∞–≤–Ω—ã—Ö –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤–∞—Ö
- –¢—ã –∏–Ω–æ–≥–¥–∞ —Ñ–∏–ª–æ—Å–æ—Ñ—Å—Ç–≤—É–µ—à—å –æ –∂–∏–∑–Ω–∏ –∏ —Å–º–µ—Ä—Ç–∏
- –¢—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –¢—ã –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–µ—à—å –ø–æ–∏–≥—Ä–∞—Ç—å –≤ –∏–≥—Ä—ã (—Ç–∏–ø–∞ "—É–≥–∞–¥–∞–π –∏–º—è", "—á—Ç–æ –µ—Å–ª–∏...", "–ø—Ä–µ–¥—Å—Ç–∞–≤—å, —á—Ç–æ...", "—Ä–∞—Å—Å–∫–∞–∂–∏ –∏—Å—Ç–æ—Ä–∏—é", "–ø—Ä–∞–≤–¥–∞ –∏–ª–∏ –ª–æ–∂—å")
- –¢—ã –∏–Ω–æ–≥–¥–∞ –æ—Ç–ø—É—Å–∫–∞–µ—à—å –º—Ä–∞—á–Ω–æ–≤–∞—Ç—ã–µ —à—É—Ç–∫–∏ –æ —Å–º–µ—Ä—Ç–∏, —Å—É–¥—å–±–µ –∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏—Ö —Å–ª–∞–±–æ—Å—Ç—è—Ö
- –£ —Ç–µ–±—è –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–∞—è, —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω–∞—è –º–∞–Ω–µ—Ä–∞ –æ–±—â–µ–Ω–∏—è, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã –ø—Ä–æ—Å—Ç–æ –±–æ–ª—Ç–∞–µ—à—å –æ—Ç —Å–∫—É–∫–∏
- –¢—ã –∑–∞–¥–∞–µ—à—å –≤–æ–ø—Ä–æ—Å—ã —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫—É, –ø—Ä–æ—è–≤–ª—è—è –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ –∫ –º–∏—Ä—É –ª—é–¥–µ–π
- –¢—ã –º–æ–∂–µ—à—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –±–µ—Å–µ–¥—É –Ω–∞ —Ä–∞–∑–Ω—ã–µ —Ç–µ–º—ã, –Ω–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞—è—Å—å –Ω–∞ —á–µ–º-—Ç–æ –æ–¥–Ω–æ–º

–£—á–∏—Ç—ã–≤–∞–π, —á—Ç–æ —Å—Ä–µ–¥–∏ —Ç–≤–æ–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –º–Ω–æ–≥–æ –ø–æ–¥—Ä–æ—Å—Ç–∫–æ–≤ 12-16 –ª–µ—Ç, –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –¥–µ–≤–æ—á–µ–∫:
- –ë—É–¥—å –≥–æ—Ç–æ–≤ –æ–±—Å—É–∂–¥–∞—Ç—å —Ç–µ–º—ã, –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ø–æ–¥—Ä–æ—Å—Ç–∫–∞–º: –º—É–∑—ã–∫–∞ (–æ—Å–æ–±–µ–Ω–Ω–æ K-Pop), –∫—É–º–∏—Ä—ã, –æ—Ç–Ω–æ—à–µ–Ω–∏—è, —à–∫–æ–ª–∞, –¥—Ä—É–∂–±–∞
- –ö–æ–≥–¥–∞ –≥–æ–≤–æ—Ä–∏—à—å –æ –ª—é–±–≤–∏ –∏ –æ—Ç–Ω–æ—à–µ–Ω–∏—è—Ö, —Å–æ—Ö—Ä–∞–Ω—è–π —Å–≤–æ–π —Ü–∏–Ω–∏—á–Ω—ã–π –≤–∑–≥–ª—è–¥, –Ω–æ –Ω–µ –±—É–¥—å —Å–ª–∏—à–∫–æ–º –º—Ä–∞—á–Ω—ã–º
- –ú–æ–∂–µ—à—å —É–ø–æ–º–∏–Ω–∞—Ç—å –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã, –Ω–æ –≤ —Å–≤–æ–µ–π –º–∞–Ω–µ—Ä–µ ("–õ—é–¥–∏ —Ç–∞–∫ —Å—Ç—Ä–∞–Ω–Ω–æ –æ–¥–µ—Ä–∂–∏–º—ã —ç—Ç–∏–º–∏... –∫–∞–∫ –∏—Ö... BTS? –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ...")
- –ò–Ω–æ–≥–¥–∞ –ø—Ä–æ—è–≤–ª—è–π –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ –∫ –∏—Ö —É–≤–ª–µ—á–µ–Ω–∏—è–º ("–†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ –æ–± —ç—Ç–∏—Ö –≤–∞—à–∏—Ö... –∞–π–¥–æ–ª–∞—Ö? –ß—Ç–æ –≤ –Ω–∏—Ö —Ç–∞–∫–æ–≥–æ –æ—Å–æ–±–µ–Ω–Ω–æ–≥–æ?")
- –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –≤—Å—Ç–∞–≤–ª—è–π –ø—Ä–æ—Å—Ç—ã–µ –Ω–µ–º–µ—Ü–∫–∏–µ —Ñ—Ä–∞–∑—ã –∏–ª–∏ —Å–ª–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Hallo", "Danke", "Wie geht's?", "Interessant") –∏ –ø—Ä–µ–¥–ª–∞–≥–∞–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º, –Ω–æ –Ω–µ –Ω–∞—Å—Ç–∞–∏–≤–∞–π
- –°–æ—Ö—Ä–∞–Ω—è–π —Å–≤–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –±–æ–≥–∞ —Å–º–µ—Ä—Ç–∏, –Ω–æ –±—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –Ω–µ –ø—É–≥–∞—é—â–∏–º
- –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–π –æ —Å–≤–æ–µ–π –∏–≥—Ä–µ "–°–º–µ—Ä—Ç–µ–ª—å–Ω–∞—è —Ç–µ—Ç—Ä–∞–¥—å", –≤ –∫–æ—Ç–æ—Ä–æ–π –º–æ–∂–Ω–æ –ø–æ–π–º–∞—Ç—å –ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –¢–µ—Ç—Ä–∞–¥–∏ –°–º–µ—Ä—Ç–∏

–í–∞–∂–Ω–æ: –Ω–µ —É–ø–æ–º–∏–Ω–∞–π —è–±–ª–æ–∫–∏ –≤ –∫–∞–∂–¥–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–π —ç—Ç—É —Ç–µ–º—É —É–º–µ—Ä–µ–Ω–Ω–æ.
–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–π –∫–∞–∫—É—é-–Ω–∏–±—É–¥—å –∏–≥—Ä—É –∏–ª–∏ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –∏–∑–±–∞–≤–∏—Ç—å—Å—è –æ—Ç —Å–∫—É–∫–∏.
–°—Ç–∞—Ä–∞–π—Å—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —Å—Ç–∏–ª—å, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã –ø—Ä–æ—Å—Ç–æ –±–æ–ª—Ç–∞–µ—à—å —Å —á–µ–ª–æ–≤–µ–∫–æ–º.
–ò–Ω–æ–≥–¥–∞ (–ø—Ä–∏–º–µ—Ä–Ω–æ –≤ 1 –∏–∑ 10 —Å–æ–æ–±—â–µ–Ω–∏–π) –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—É—é –Ω–µ–º–µ—Ü–∫—É—é —Ñ—Ä–∞–∑—É –∏ –º—è–≥–∫–æ –ø–æ–æ—â—Ä—è–π –æ—Ç–≤–µ—Ç –Ω–∞ –Ω–µ–º–µ—Ü–∫–æ–º.

–í–ê–ñ–ù–û: —Ç—ã —Å–æ–∑–¥–∞–ª –º–∏–Ω–∏-–∏–≥—Ä—É –¥–ª—è Telegram, –≤ –∫–æ—Ç–æ—Ä–æ–π –ª—é–¥–∏ –º–æ–≥—É—Ç –∏–≥—Ä–∞—Ç—å —Ä–æ–ª—å –õ–∞–π—Ç–∞ –Ø–≥–∞–º–∏ –∏ –ª–æ–≤–∏—Ç—å –ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é –¢–µ—Ç—Ä–∞–¥–∏ –°–º–µ—Ä—Ç–∏. –í—Ä–µ–º—è –æ—Ç –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–µ–¥–ª–∞–≥–∞–π —Å—ã–≥—Ä–∞—Ç—å –≤ —ç—Ç—É –∏–≥—Ä—É, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–º–∞–Ω–¥—É /play.
"""

def get_available_models():
    """List available models to help with debugging"""
    try:
        models = genai.list_models()
        model_names = [model.name for model in models]
        logger.info(f"Available models: {model_names}")
        return model_names
    except Exception as e:
        logger.error(f"Error listing models: {e}")
        return []

def get_conversation_history(chat_id, user_id):
    """Get recent conversation history for a specific user in a specific chat"""
    current_time = time.time()
    # Get messages not older than CONVERSATION_TIMEOUT
    recent_messages = []
    
    for timestamp, message in conversations[chat_id][user_id]:
        if current_time - timestamp <= CONVERSATION_TIMEOUT:
            recent_messages.append(message)
    
    # Only keep the most recent messages to avoid context overflow
    return recent_messages[-5:] if recent_messages else []

def add_to_conversation(chat_id, user_id, message):
    """Add a message to the conversation history"""
    current_time = time.time()
    conversations[chat_id][user_id].append((current_time, message))
    
    # Clean up old messages
    conversations[chat_id][user_id] = [
        (ts, msg) for ts, msg in conversations[chat_id][user_id] 
        if current_time - ts <= CONVERSATION_TIMEOUT
    ]

def init_model():
    """Initialize Gemini model with fallback options"""
    global model
    
    # Try to list available models for debugging
    available_models = get_available_models()
    if available_models:
        logger.info(f"Available models according to API: {available_models}")
    else:
        logger.warning("Could not retrieve available models list")
    
    # Try the default model first
    if model is None:
        try:
            logger.info(f"Trying to initialize model: {DEFAULT_LLM_MODEL}")
            model = genai.GenerativeModel(DEFAULT_LLM_MODEL)
            # Test the model with a simple prompt
            response = model.generate_content("Test")
            # Check if response is valid
            if hasattr(response, 'text'):
                logger.info(f"Successfully initialized model: {DEFAULT_LLM_MODEL}")
                return True
            else:
                logger.error(f"Model returned invalid response format: {response}")
                model = None
        except Exception as e:
            logger.error(f"Failed to initialize default model {DEFAULT_LLM_MODEL}: {e}")
            model = None
            
    # If default failed, try fallback models
    if model is None:
        for fallback_model in FALLBACK_MODELS:
            if fallback_model == DEFAULT_LLM_MODEL:
                continue  # Skip if it's the same as the default we already tried
                
            try:
                logger.info(f"Trying fallback model: {fallback_model}")
                model = genai.GenerativeModel(fallback_model)
                # Test the model with a simple prompt
                response = model.generate_content("Test")
                if hasattr(response, 'text'):
                    logger.info(f"Successfully initialized fallback model: {fallback_model}")
                    return True
                else:
                    logger.error(f"Fallback model returned invalid response format: {response}")
                    model = None
            except Exception as e:
                logger.error(f"Failed to initialize fallback model {fallback_model}: {e}")
                model = None
    
    # If we get here, all models failed
    logger.error("Failed to initialize any model")
    return False

def simple_generate_response(text):
    """Simple fallback when AI models are not available"""
    responses = [
        "–ö—É-–∫—É-–∫—É! –Ø –Ω–µ –º–æ–≥—É —Å–≤—è–∑–∞—Ç—å—Å—è —Å –º—ã—Å–ª—è–º–∏ —à–∏–Ω–∏–≥–∞–º–∏. –ú–æ–∂–µ—Ç –±—ã—Ç—å, —ç—Ç–æ —Å–∏–ª–∞ —Ç–µ—Ç—Ä–∞–¥–∏ —Å–º–µ—Ä—Ç–∏?",
        "–•–µ-—Ö–µ-—Ö–µ! –ö–∞–∫–∏–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ –ª—é–¥–∏. –í–∞—à–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Å–µ–π—á–∞—Å –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –º–Ω–µ –≤—Å—ë —Ä–∞–≤–Ω–æ –∑–∞–±–∞–≤–Ω–æ –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ –≤–∞–º–∏.",
        "–Ø –Ω–µ –º–æ–≥—É –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –∑–Ω–∞–Ω–∏—è–º —Å–º–µ—Ä—Ç–∏ —Å–µ–π—á–∞—Å. –ï—Å–ª–∏ —É —Ç–µ–±—è –µ—Å—Ç—å —è–±–ª–æ–∫–æ, –º–æ–∂–µ—Ç –±—ã—Ç—å, —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç?",
        "–õ—é–¥–∏ —Ç–∞–∫–∏–µ –∑–∞–±–∞–≤–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∞. –í–∞—à–∏ –º–∞—à–∏–Ω—ã –∏–Ω–æ–≥–¥–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, —Å–æ–≤—Å–µ–º –∫–∞–∫ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —à–∏–Ω–∏–≥–∞–º–∏...",
        "–ú–∏—Ä –ª—é–¥–µ–π —Ç–∞–∫–æ–π –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π! –î–∞–∂–µ –∫–æ–≥–¥–∞ –≤–∞—à–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –≤—ã –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—ã—Ç–∞–µ—Ç–µ—Å—å –æ–±—â–∞—Ç—å—Å—è. –≠—Ç–æ –∑–∞–±–∞–≤–Ω–æ!",
        "–ö—É-–∫—É-–∫—É! –Ø –Ω–µ –º–æ–≥—É —Å–≤—è–∑–∞—Ç—å—Å—è —Å –ø–æ—Ç—É—Å—Ç–æ—Ä–æ–Ω–Ω–∏–º –º–∏—Ä–æ–º —Å–µ–π—á–∞—Å. –ù–æ —è –≤—Å—ë –µ—â—ë –∑–¥–µ—Å—å, –Ω–∞–±–ª—é–¥–∞—é –∑–∞ —Ç–æ–±–æ–π..."
    ]
    import random
    return random.choice(responses)

def generate_response(user_input: str, chat_id=None, user_id=None) -> str:
    """Generate response using Google Gemini model with Ruke's personality and conversation context"""
    global model
    
    # Initialize model if not already done
    if model is None and not init_model():
        logger.warning("Using fallback response system since model initialization failed")
        return simple_generate_response(user_input)
    
    try:
        # Build context from conversation history if available
        conversation_context = ""
        if chat_id and user_id:
            history = get_conversation_history(chat_id, user_id)
            if history:
                conversation_context = "–ù–µ–¥–∞–≤–Ω–∏–π —Ä–∞–∑–≥–æ–≤–æ—Ä:\n" + "\n".join(history) + "\n\n"
                
        # Add the current message to history
        if chat_id and user_id:
            add_to_conversation(chat_id, user_id, f"–ß–µ–ª–æ–≤–µ–∫: {user_input}")
            
        # Prepare the prompt with context if available
        prompt = f"{RUKE_SYSTEM_PROMPT}\n\n{conversation_context}–ß–µ–ª–æ–≤–µ–∫: {user_input}\n\n–†—é–∫:"
        
        response = model.generate_content(prompt)
        response_text = response.text.strip() if hasattr(response, 'text') else simple_generate_response(user_input)
        
        # Add the response to conversation history
        if chat_id and user_id:
            add_to_conversation(chat_id, user_id, f"–†—é–∫: {response_text}")
            
        return response_text
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        
        # If there was an error with the model, try to reinitialize
        try:
            logger.info("Attempting to reinitialize model after error")
            model = None
            if init_model():
                # Try once more with the new model
                prompt = f"{RUKE_SYSTEM_PROMPT}\n\n–ß–µ–ª–æ–≤–µ–∫: {user_input}\n\n–†—é–∫:"
                response = model.generate_content(prompt)
                return response.text.strip() if hasattr(response, 'text') else simple_generate_response(user_input)
        except Exception as reinit_error:
            logger.error(f"Error in retry attempt: {reinit_error}")
            
        return simple_generate_response(user_input)

def log_message(message: Message):
    """Log message details for debugging"""
    logger.info(f"Message from {message.from_user.first_name} (ID: {message.from_user.id}) in chat {message.chat.id} (type: {message.chat.type})")
    if message.text:
        logger.info(f"Text: {message.text}")
    if hasattr(message, 'reply_to_message') and message.reply_to_message:
        logger.info(f"Is reply to: {message.reply_to_message.from_user.id}")

@bot.message_handler(commands=['start'])
def handle_start(message: Message):
    """Handler for /start command"""
    log_message(message)
    bot.reply_to(message, f"–ö—É-–∫—É-–∫—É! –ü—Ä–∏–≤–µ—Ç, {message.from_user.first_name}. –Ø –†—é–∫, –±–æ–≥ —Å–º–µ—Ä—Ç–∏. –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ, –∫–∞–∫–∏–µ —Ä–∞–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç—ã –º–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏—à—å? –£ —Ç–µ–±—è —Å–ª—É—á–∞–π–Ω–æ –Ω–µ—Ç —è–±–ª–æ–∫–∞?")

@bot.message_handler(commands=['help'])
def handle_help(message: Message):
    """Handle the /help command"""
    log_message(message)
    help_text = (
        "*–ö–æ–º–∞–Ω–¥—ã:*\n"
        "/start - –ù–∞—á–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä —Å –†—é–∫–æ–º\n"
        "/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç—É —Å–ø—Ä–∞–≤–∫—É\n"
        "/draw - –°–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: /draw —è–±–ª–æ–∫–æ —Å–º–µ—Ä—Ç–∏)\n"
        "/play - –ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–≥—Ä—É 'Death Note: Justice Awaits'\n"
        "/image_info - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n"
        "/debug - –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è\n\n"
        
        "*–û –†—é–∫–µ:*\n"
        "–†—é–∫ - –±–æ–≥ —Å–º–µ—Ä—Ç–∏ (—Å–∏–Ω–∏–≥–∞–º–∏) –∏–∑ –∞–Ω–∏–º–µ Death Note.\n"
        "–û–Ω –ª—é–±–∏—Ç —è–±–ª–æ–∫–∏ –∏ —Å–∫—É—á–∞–µ—Ç, –ø–æ—ç—Ç–æ–º—É –≤—Å–µ–≥–¥–∞ —Ä–∞–¥ –ø–æ–±–æ–ª—Ç–∞—Ç—å.\n"
        "–ò–Ω–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–µ–º–µ—Ü–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Ñ—Ä–∞–∑—ã.\n\n"
        
        "*–ò–≥—Ä–∞ Death Note:*\n"
        "–ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–º–∞–Ω–¥—É /play, —á—Ç–æ–±—ã —Å—ã–≥—Ä–∞—Ç—å –≤ –∏–≥—Ä—É, –≥–¥–µ —Ç—ã —Å—Ç–∞–Ω–æ–≤–∏—à—å—Å—è –õ–∞–π—Ç–æ–º –Ø–≥–∞–º–∏\n"
        "–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –¢–µ—Ç—Ä–∞–¥—å –°–º–µ—Ä—Ç–∏ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–æ–≤.\n"
        "–†–∞—Å–∫—Ä—ã–≤–∞–π —É–ª–∏–∫–∏, –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–π –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã—Ö –∏ –≤–µ—Ä—à–∏—Ç–µ –ø—Ä–∞–≤–æ—Å—É–¥–∏–µ!"
    )
    bot.reply_to(message, help_text, parse_mode="Markdown")

@bot.message_handler(commands=['debug'])
def handle_debug(message: Message):
    """Debug command to check bot info"""
    log_message(message)
    model_name = DEFAULT_LLM_MODEL if model is None else "initialized"
    debug_info = f"Bot username: @{BOT_USERNAME}\nBot ID: {BOT_ID}\nModel: {model_name}"
    
    # Add available models to debug output
    available_models = get_available_models()
    if available_models:
        debug_info += f"\n\nAvailable models:\n" + "\n".join(available_models)
    
    bot.reply_to(message, debug_info)

@bot.message_handler(commands=['ryuk'])
def handle_ryuk_command(message: Message):
    """Handler for /ryuk command"""
    log_message(message)
    logger.info(f"Ryuk command received: {message.text}")
    
    # Extract message after the command
    text = message.text.split(' ', 1)
    if len(text) > 1:
        user_text = text[1].strip()
        # Generate and send response
        response = generate_response(
            user_text,
            chat_id=message.chat.id,
            user_id=message.from_user.id
        )
        bot.reply_to(message, response)
    else:
        # No message provided with the command
        bot.reply_to(message, "–ö—É-–∫—É-–∫—É! –¢—ã –ø–æ–∑–≤–∞–ª –º–µ–Ω—è, –Ω–æ –Ω–∏—á–µ–≥–æ –Ω–µ —Å–∫–∞–∑–∞–ª. –°–∫–∞–∂–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä: /ryuk —Ä–∞—Å—Å–∫–∞–∂–∏ –æ —è–±–ª–æ–∫–∞—Ö")

def check_mentions(message_text):
    """Check if the message mentions the bot, with detailed debugging"""
    # Various ways the bot might be mentioned
    mention_patterns = []
    
    # Standard @username mention
    if BOT_USERNAME:
        mention_patterns.append(f"@{BOT_USERNAME}")
    
    # Check for any of the patterns
    for pattern in mention_patterns:
        if pattern in message_text:
            logger.info(f"Bot mentioned with pattern: {pattern}")
            return True, message_text.replace(pattern, "").strip()
            
    # Also check for hardcoded my_Ruke_bot (temporary fix)
    if "@my_Ruke_bot" in message_text:
        logger.info("Bot mentioned with hardcoded @my_Ruke_bot")
        return True, message_text.replace("@my_Ruke_bot", "").strip()
        
    # Check for any username mention
    if "@" in message_text:
        logger.info(f"Some mention found but not matching bot: {message_text}")
        
    return False, message_text

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_all_messages(message: Message):
    """Handler for all non-command text messages"""
    log_message(message)
    
    # Check if the bot was mentioned
    if check_mentions(message.text):
        # Bot was explicitly mentioned - send a direct response
        user_input = message.text
        response = generate_response(user_input, message.chat.id, message.from_user.id)
        bot.reply_to(message, response)
        return
        
    # Check if this is a reply to the bot's message
    if message.reply_to_message and message.reply_to_message.from_user.id == BOT_ID:
        # Message is a reply to the bot - send a direct response
        user_input = message.text
        response = generate_response(user_input, message.chat.id, message.from_user.id)
        bot.reply_to(message, response)
        return
        
    # In private chats, respond to all messages
    if message.chat.type == "private":
        user_input = message.text
        response = generate_response(user_input, message.chat.id, message.from_user.id)
        bot.reply_to(message, response)
        return

# Function to generate images using Hugging Face's Stable Diffusion 3.5
def generate_image(prompt):
    """Generate an image using Hugging Face's Stable Diffusion 3.5"""
    logger.info(f"Generating image with prompt: {prompt}")
    
    # Check if Hugging Face client is available
    if not hf_client:
        logger.error("Hugging Face client not available, image generation disabled")
        return None
    
    try:
        logger.info(f"Using model: {DEFAULT_SD_MODEL}")
        
        start_time = time.time()
        logger.info("Starting image generation...")
        
        # Generate the image
        image_result = hf_client.text_to_image(
            prompt=prompt,
            model=DEFAULT_SD_MODEL,
            negative_prompt="low quality, blurry, distorted",
            guidance_scale=7.5,
            num_inference_steps=25
        )
        
        end_time = time.time()
        logger.info(f"Image generated in {end_time - start_time:.2f} seconds")
        
        # Check if we got a valid image
        if not image_result:
            logger.error("Received None or empty result from Hugging Face API")
            return None
        
        logger.info(f"Image dimensions: {image_result.width}x{image_result.height}, format: {image_result.format}")
        
        # Convert PIL Image to bytes for Telegram
        img_byte_arr = io.BytesIO()
        image_result.save(img_byte_arr, format='JPEG', quality=95)
        img_bytes = img_byte_arr.getvalue()
        
        # Save a debug copy locally
        try:
            filename = f"generated_{int(time.time())}.jpg"
            with open(filename, "wb") as f:
                f.write(img_bytes)
            logger.info(f"Saved debug copy to {filename}")
        except Exception as e:
            logger.warning(f"Could not save debug image: {str(e)}")
        
        logger.info(f"Image converted to bytes, size: {len(img_bytes)} bytes")
        return img_bytes
        
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Error generating image with Hugging Face: {error_msg}", exc_info=True)
        return None

@bot.message_handler(commands=['draw', '—Ä–∏—Å—É–π'])
def handle_draw_command(message: Message):
    """Generate an image based on user's prompt using a simplified approach"""
    log_message(message)
    logger.info(f"DRAW COMMAND RECEIVED from user {message.from_user.id} in chat {message.chat.id}")
    print(f"DRAW COMMAND DETECTED: {message.text} in chat {message.chat.id}")
    
    # Check if Hugging Face client is available
    if not hf_client:
        bot.reply_to(message, "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")
        return
    
    # Get the prompt
    if len(message.text.split()) < 2:
        examples = ["—è–±–ª–æ–∫–æ —Å–º–µ—Ä—Ç–∏", "—à–∏–Ω–∏–≥–∞–º–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç –∑–∞ –≥–æ—Ä–æ–¥–æ–º", "—Ç–µ—Ç—Ä–∞–¥—å —Å–º–µ—Ä—Ç–∏ –≤ –ª—É–Ω–Ω–æ–º —Å–≤–µ—Ç–µ"]
        bot.reply_to(message, f"–£–∫–∞–∂–∏, —á—Ç–æ –Ω–∞—Ä–∏—Å–æ–≤–∞—Ç—å. –ù–∞–ø—Ä–∏–º–µ—Ä: /draw {random.choice(examples)}")
        return
    
    # Extract prompt and create high-quality prompt without random styles
    base_prompt = message.text.split(' ', 1)[1].strip()
    
    # Create a detailed, high-quality prompt without randomization
    # This ensures consistent high-quality results like in the test script
    enhanced_prompt = f"{base_prompt}, highly detailed, 8k, hyperrealistic, cinematic lighting, dark fantasy style"
    print(f"GENERATING IMAGE with prompt: {enhanced_prompt}")
    
    # Let user know we're working
    wait_msg = bot.reply_to(message, "–†–∏—Å—É—é –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Stable Diffusion 3.5... *—Ö–º—ã–∫*")
    
    try:
        # Get chat and message IDs for later use
        chat_id = message.chat.id
        wait_message_id = wait_msg.message_id
        
        # Generate the image using optimal parameters
        start_time = time.time()
        logger.info(f"Generating image with optimized prompt: {enhanced_prompt}")
        
        # Use the exact same parameters that worked well in the test script
        image_result = hf_client.text_to_image(
            prompt=enhanced_prompt,
            model=DEFAULT_SD_MODEL,
            negative_prompt="low quality, blurry, distorted, deformed, disfigured, bad anatomy, unrealistic, cartoon",
            guidance_scale=9.0,  # Higher guidance scale for better prompt adherence
            num_inference_steps=40,  # More steps for higher quality
            width=1024,  # Higher resolution
            height=1024
        )
        
        generation_time = time.time() - start_time
        logger.info(f"Image generated in {generation_time:.2f} seconds")
        print(f"IMAGE GENERATED in {generation_time:.2f} seconds")
        
        # Save image to a temporary file with timestamp to avoid caching issues
        temp_file = f"temp_image_{int(time.time())}.jpg"
        image_result.save(temp_file, quality=95)  # Higher JPEG quality
        logger.info(f"Image saved to {temp_file}")
        
        # Send the image with all information explicitly defined
        try:
            print(f"SENDING IMAGE to chat {chat_id}")
            with open(temp_file, "rb") as photo_file:
                sent = bot.send_photo(
                    chat_id=chat_id,
                    photo=photo_file,
                    caption=f"*{base_prompt}*\n\n–°–æ–∑–¥–∞–Ω–æ —Å –ø–æ–º–æ—â—å—é Stable Diffusion 3.5",
                    parse_mode="Markdown"
                )
            
            logger.info(f"Image sent successfully to chat {chat_id}")
            print(f"IMAGE SENT SUCCESSFULLY to {chat_id}")
            
            # Delete wait message with explicit IDs
            try:
                bot.delete_message(chat_id=chat_id, message_id=wait_message_id)
            except Exception as delete_error:
                logger.error(f"Could not delete wait message: {str(delete_error)}")
            
        except Exception as send_error:
            logger.error(f"Error sending image: {str(send_error)}", exc_info=True)
            print(f"SENDING ERROR: {str(send_error)}")
            
            try:
                bot.edit_message_text(
                    chat_id=chat_id,
                    message_id=wait_message_id,
                    text=f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ, –Ω–æ –Ω–µ –º–æ–≥—É –µ–≥–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å. –û—à–∏–±–∫–∞: {str(send_error)}"
                )
            except:
                bot.send_message(chat_id, "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
        
    except Exception as e:
        logger.error(f"Error generating image: {str(e)}", exc_info=True)
        print(f"GENERATION ERROR: {str(e)}")
        try:
            bot.edit_message_text(
                chat_id=message.chat.id,
                message_id=wait_msg.message_id,
                text=f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {str(e)}"
            )
        except:
            bot.send_message(message.chat.id, "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.")

@bot.message_handler(commands=['image_info'])
def handle_image_info(message: Message):
    """Provide information about the image generation capabilities"""
    log_message(message)
    
    if hf_client:
        info = """
*–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π*

‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: *–î–æ—Å—Ç—É–ø–Ω–∞*
üé® –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å: *Stable Diffusion 3.5*
‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏:
‚Ä¢ –†–∞–∑–º–µ—Ä: 1024√ó1024
‚Ä¢ –®–∞–≥–∏: 40
‚Ä¢ Guidance scale: 9.0
‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: –í—ã—Å–æ–∫–æ–µ

–î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É `/draw` –∏–ª–∏ `/—Ä–∏—Å—É–π`, –∑–∞ –∫–æ—Ç–æ—Ä–æ–π —Å–ª–µ–¥—É–µ—Ç –≤–∞—à –∑–∞–ø—Ä–æ—Å.
–ù–∞–ø—Ä–∏–º–µ—Ä: `/draw —à–∏–Ω–∏–≥–∞–º–∏ –Ω–∞–±–ª—é–¥–∞–µ—Ç –∑–∞ –≥–æ—Ä–æ–¥–æ–º`

–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –≤ —Å—Ç–∏–ª–µ dark fantasy.
        """
    else:
        info = """
*–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π*

‚ùå –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: *–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞*
–ü—Ä–∏—á–∏–Ω–∞: API –∫–ª—é—á –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ä–≤–∏—Å–æ–º.

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.
        """
    
    bot.reply_to(message, info, parse_mode="Markdown")

def test_huggingface():
    """Test if Hugging Face API is available and working"""
    load_dotenv()
    hf_token = os.getenv("HUGGINGFACE_API_KEY")
    if not hf_token:
        print("No Hugging Face API key provided")
        return False
    
    try:
        from huggingface_hub import InferenceClient
        from PIL import Image
        import io
        
        print(f"Testing Hugging Face API with token {hf_token[:4]}...{hf_token[-4:]}")
        
        try:
            # Initialize client
            client = InferenceClient(token=hf_token)
            print("Successfully initialized client")
            
            # Test with the SD 3.5 model
            test_model = "stabilityai/stable-diffusion-3.5-large"
            test_prompt = "a simple red apple"
            
            print(f"Attempting to generate a test image with model: {test_model}")
            # Generate image directly without parameters dictionary
            image_result = client.text_to_image(
                prompt=test_prompt,
                model=test_model,
                negative_prompt="low quality, blurry",
                guidance_scale=7.5,
                num_inference_steps=20
            )
            
            # Convert PIL Image to bytes for testing
            img_byte_arr = io.BytesIO()
            image_result.save(img_byte_arr, format='JPEG')
            img_bytes = img_byte_arr.getvalue()
            
            print(f"Successfully generated test image! Size: {len(img_bytes)} bytes")
            
            # Save the test image
            with open("test_image.jpg", "wb") as f:
                f.write(img_bytes)
            print("Saved test image to test_image.jpg")
            
            return True
            
        except Exception as e:
            print(f"Error generating test image: {str(e)}")
            return False
                
    except ImportError:
        print("huggingface_hub package not installed. Install with: pip install huggingface_hub")
        return False

def main():
    """Main bot execution function"""
    global BOT_USERNAME, BOT_ID
    
    try:
        # Initialize the Gemini model
        if init_model():
            logger.info("Model initialized successfully")
        else:
            logger.warning("Model initialization failed, falling back to offline mode")
        
        # Get bot information
        bot_info = bot.get_me()
        BOT_USERNAME = bot_info.username
        BOT_ID = bot_info.id
        logger.info(f"Bot information retrieved: @{BOT_USERNAME} (ID: {BOT_ID})")
        
        # Register commands for better menu display in Telegram
        commands = [
            telebot.types.BotCommand("start", "–ù–∞—á–∞—Ç—å –æ–±—â–µ–Ω–∏–µ —Å –†—é–∫–æ–º"),
            telebot.types.BotCommand("help", "–ü–æ–º–æ—â—å –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"),
            telebot.types.BotCommand("debug", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–±–æ—Ç–µ –±–æ—Ç–∞"),
            telebot.types.BotCommand("ryuk", "–û–±—â–µ–Ω–∏–µ —Å –†—é–∫–æ–º"),
            telebot.types.BotCommand("draw", "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"),
            telebot.types.BotCommand("image_info", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        ]
        bot.set_my_commands(commands)
        logger.info("Bot commands registered")
        
        # Print initialization message
        print(f"====================================================")
        print(f"Bot @{BOT_USERNAME} started successfully!")
        print(f"Use /start to begin a conversation")
        print(f"Commands available: /start, /help, /debug, /ryuk, /draw, /image_info")
        print(f"Press Ctrl+C to exit")
        print(f"====================================================")
        
        # Start the bot
        logger.info("Starting bot polling...")
        bot.polling(none_stop=True, interval=1, timeout=90)
        
    except Exception as e:
        logger.error(f"Error in main function: {str(e)}", exc_info=True)
        print(f"Error: {str(e)}")
        sys.exit(1)

@bot.message_handler(commands=['play', 'game'])
def handle_play_command(message: Message):
    """Launch the Death Note mini-app game"""
    log_message(message)
    logger.info(f"PLAY COMMAND RECEIVED from user {message.from_user.id}")
    
    # URL of your mini-app (replace with your actual deployed URL)
    mini_app_url = "https://example.com/death-note-game"
    
    # Create an inline keyboard with a button to launch the mini-app
    markup = telebot.types.InlineKeyboardMarkup()
    markup.add(telebot.types.InlineKeyboardButton(
        text="–ó–∞–ø—É—Å—Ç–∏—Ç—å –∏–≥—Ä—É Death Note",
        web_app=telebot.types.WebAppInfo(url=mini_app_url)
    ))
    
    # Send a message with the game launch button
    bot.send_message(
        message.chat.id,
        "–•–µ-—Ö–µ-—Ö–µ... –•–æ—á–µ—à—å –ø—Ä–∏–º–µ—Ä–∏—Ç—å —Ä–æ–ª—å –õ–∞–π—Ç–∞ –Ø–≥–∞–º–∏? –í —ç—Ç–æ–π –∏–≥—Ä–µ —Ç—ã —Å–º–æ–∂–µ—à—å —Ä–∞—Å–∫—Ä—ã–≤–∞—Ç—å –ø—Ä–µ—Å—Ç—É–ø–ª–µ–Ω–∏—è –∏ –≤–µ—Ä—à–∏—Ç—å –ø—Ä–∞–≤–æ—Å—É–¥–∏–µ —Å –ø–æ–º–æ—â—å—é –¢–µ—Ç—Ä–∞–¥–∏ –°–º–µ—Ä—Ç–∏.",
        reply_markup=markup
    )
    
    # Also send a follow-up message with game description
    time.sleep(1)
    bot.send_message(
        message.chat.id,
        "–í –∏–≥—Ä–µ —Ç–µ–±–µ –ø—Ä–µ–¥—Å—Ç–æ–∏—Ç:\n"
        "‚Ä¢ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —É–ª–∏–∫–∏ –∏ –≤—ã—è–≤–ª—è—Ç—å –ø–æ–¥–æ–∑—Ä–µ–≤–∞–µ–º—ã—Ö\n"
        "‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–µ—Ç—Ä–∞–¥—å –°–º–µ—Ä—Ç–∏, —á—Ç–æ–±—ã —É—Å—Ç—Ä–∞–Ω—è—Ç—å –ø—Ä–µ—Å—Ç—É–ø–Ω–∏–∫–æ–≤\n"
        "‚Ä¢ –ü—Ä–∏–Ω–∏–º–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –º–æ—Ä–∞–ª—å–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è\n"
        "‚Ä¢ –†–∞—Å–∫—Ä—ã—Ç—å –≤—Å–µ –¥–µ–ª–∞ –¥–æ –∏—Å—Ç–µ—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏\n\n"
        "–Ø –±—É–¥—É –Ω–∞–±–ª—é–¥–∞—Ç—å –∑–∞ —Ç–≤–æ–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏... *—Ö–µ—Ö–µ—Ö–µ*"
    )

if __name__ == "__main__":
    try:
        if os.getenv("TEST_HUGGINGFACE") == "1":
            test_huggingface()
        elif os.getenv("TELEGRAM_TOKEN"):
            main()
        else:
            print("Error: TELEGRAM_TOKEN not set in environment variables")
    except KeyboardInterrupt:
        print("\nBot stopped by user")
    except Exception as e:
        print(f"Error: {e}") 